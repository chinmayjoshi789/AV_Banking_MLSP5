{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RandomizedSearchCV,cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestRegressor,BaggingRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import r2_score,roc_auc_score,classification_report,mean_squared_error,accuracy_score,confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('D:/datasets+minipro/analytics vidhya datasets/janatahack p5 banking/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Loan_Amount_Requested</th>\n",
       "      <th>Length_Employed</th>\n",
       "      <th>Home_Owner</th>\n",
       "      <th>Annual_Income</th>\n",
       "      <th>Income_Verified</th>\n",
       "      <th>Purpose_Of_Loan</th>\n",
       "      <th>Debt_To_Income</th>\n",
       "      <th>Inquiries_Last_6Mo</th>\n",
       "      <th>Months_Since_Deliquency</th>\n",
       "      <th>Number_Open_Accounts</th>\n",
       "      <th>Total_Accounts</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Interest_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000001</td>\n",
       "      <td>7,000</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>Rent</td>\n",
       "      <td>68000.0</td>\n",
       "      <td>not verified</td>\n",
       "      <td>car</td>\n",
       "      <td>18.37</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Loan_Amount_Requested Length_Employed Home_Owner  Annual_Income  \\\n",
       "0  10000001                 7,000        < 1 year       Rent        68000.0   \n",
       "\n",
       "  Income_Verified Purpose_Of_Loan  Debt_To_Income  Inquiries_Last_6Mo  \\\n",
       "0    not verified             car           18.37                   0   \n",
       "\n",
       "   Months_Since_Deliquency  Number_Open_Accounts  Total_Accounts  Gender  \\\n",
       "0                      NaN                     9              14  Female   \n",
       "\n",
       "   Interest_Rate  \n",
       "0              1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 164309 entries, 0 to 164308\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Loan_ID                  164309 non-null  int64  \n",
      " 1   Loan_Amount_Requested    164309 non-null  object \n",
      " 2   Length_Employed          156938 non-null  object \n",
      " 3   Home_Owner               138960 non-null  object \n",
      " 4   Annual_Income            139207 non-null  float64\n",
      " 5   Income_Verified          164309 non-null  object \n",
      " 6   Purpose_Of_Loan          164309 non-null  object \n",
      " 7   Debt_To_Income           164309 non-null  float64\n",
      " 8   Inquiries_Last_6Mo       164309 non-null  int64  \n",
      " 9   Months_Since_Deliquency  75930 non-null   float64\n",
      " 10  Number_Open_Accounts     164309 non-null  int64  \n",
      " 11  Total_Accounts           164309 non-null  int64  \n",
      " 12  Gender                   164309 non-null  object \n",
      " 13  Interest_Rate            164309 non-null  int64  \n",
      "dtypes: float64(3), int64(5), object(6)\n",
      "memory usage: 17.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['7,000', '30,000', '24,725', ..., '28,950', '33,325', '29,825'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem 1\n",
    "df['Loan_Amount_Requested'].unique() \n",
    "\n",
    "# No Null values,Need to remove ',', convert data type to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in feature Length_Employed:\n",
      " ['< 1 year' '4 years' '7 years' '8 years' '2 years' '10+ years' '1 year'\n",
      " nan '6 years' '9 years' '3 years' '5 years'] \n",
      "\n",
      "null values in Length_Employed: 7371\n"
     ]
    }
   ],
   "source": [
    "# Problem 2:\n",
    "print('unique values in feature Length_Employed:\\n',df['Length_Employed'].unique(),'\\n')\n",
    "print('null values in Length_Employed:',df['Length_Employed'].isnull().sum())\n",
    "\n",
    "#need to impute null values 7371,convert datatype to int,remove years & < symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in feature Home_Owner:\n",
      " ['Rent' 'Mortgage' nan 'Own' 'Other' 'None'] \n",
      "\n",
      "null values in Home_Owner: 25349\n"
     ]
    }
   ],
   "source": [
    "# Problem 3:\n",
    "print('unique values in feature Home_Owner:\\n',df['Home_Owner'].unique(),'\\n')\n",
    "print('null values in Home_Owner:',df['Home_Owner'].isnull().sum())\n",
    "\n",
    "#need to impute null values 25349,Impute None class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in feature Annual_Income:\n",
      " [68000.      nan 75566.4 ... 51024.  28721.4 24864. ] \n",
      "\n",
      "null values in Annual_Income: 25102\n"
     ]
    }
   ],
   "source": [
    "# Problem 4:\n",
    "print('unique values in feature Annual_Income:\\n',df['Annual_Income'].unique(),'\\n')\n",
    "print('null values in Annual_Income:',df['Annual_Income'].isnull().sum())\n",
    "\n",
    "#Need to impute null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in feature Purpose_Of_Loan:\n",
      " ['car' 'debt_consolidation' 'credit_card' 'home_improvement'\n",
      " 'major_purchase' 'other' 'medical' 'small_business' 'moving' 'wedding'\n",
      " 'vacation' 'house' 'educational' 'renewable_energy'] \n",
      "\n",
      "null values in Purpose_Of_Loan: 0\n"
     ]
    }
   ],
   "source": [
    "# Problem 5:\n",
    "print('unique values in feature Purpose_Of_Loan:\\n',df['Purpose_Of_Loan'].unique(),'\\n')\n",
    "print('null values in Purpose_Of_Loan:',df['Purpose_Of_Loan'].isnull().sum())\n",
    "#No issues :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in feature Debt_To_Income:\n",
      " [18.37 14.93 15.88 ... 38.03 39.04 35.06] \n",
      "\n",
      "null values in Debt_To_Income: 0\n"
     ]
    }
   ],
   "source": [
    "# Problem 6:\n",
    "print('unique values in feature Debt_To_Income:\\n',df['Debt_To_Income'].unique(),'\\n')\n",
    "print('null values in Debt_To_Income:',df['Debt_To_Income'].isnull().sum())\n",
    "#No issues :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in feature Inquiries_Last_6Mo:\n",
      " [0 3 1 2 4 5 6 7 8] \n",
      "\n",
      "null values in Inquiries_Last_6Mo: 0\n"
     ]
    }
   ],
   "source": [
    "# Problem 7:\n",
    "print('unique values in feature Inquiries_Last_6Mo:\\n',df['Inquiries_Last_6Mo'].unique(),'\\n')\n",
    "print('null values in Inquiries_Last_6Mo:',df['Inquiries_Last_6Mo'].isnull().sum())\n",
    "#No issues :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in feature Months_Since_Deliquency:\n",
      " [ nan  17.  16.  68.  13.   6.  64.  10.  63.  33.  22.  28.   8.  26.\n",
      "  41.  46.  62.  21.  12.  52.   3.  65.  43.  53.  44.   1.  11.  71.\n",
      "  23.  15.  48.  39.  30.  32.  18.  24.  47.  66.  19.  76.   4.  29.\n",
      "  45.  38.  56.  37.  20.  35.   2.   9.  34.  14.  59.  36.  50.  81.\n",
      "  72.  69.  57.  25.   7.  49.  31.  78.  70.  27.   5.  51.  58.  75.\n",
      "  42.  74.  40.  67.  61.  55.  77.  54.   0.  60.  73.  79.  82.  80.\n",
      "  83.  88. 127. 106.  99.  97. 139.  96. 119. 116.  94. 141.  86.  91.\n",
      "  84.  85.  95.  90.  87. 103. 101. 121. 148.  93. 122.  92. 180. 105.\n",
      " 110. 129. 130. 135. 114. 102. 111.  98. 131. 107. 170.] \n",
      "\n",
      "null values in Months_Since_Deliquency: 88379\n"
     ]
    }
   ],
   "source": [
    "# Problem 8:\n",
    "print('unique values in feature Months_Since_Deliquency:\\n',df['Months_Since_Deliquency'].unique(),'\\n')\n",
    "print('null values in Months_Since_Deliquency:',df['Months_Since_Deliquency'].isnull().sum())\n",
    "# impute null values 88379,try to convert it to try with this feature as is,years[maybe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in feature Number_Open_Accounts:\n",
      " [ 9 12 16 19 25  8 24  7  5 20 10 15  6 14 11 29  4 13 21  3 22 18 26 17\n",
      "  2 27 23 31 32 33 35 28 36 30 34 44 37 39  1 40  0 38 62 42 43 41 49 47\n",
      " 50 45 48 58 53 76 46 54 51 52] \n",
      "\n",
      "null values in Number_Open_Accounts: 0\n"
     ]
    }
   ],
   "source": [
    "# Problem 9:\n",
    "print('unique values in feature Number_Open_Accounts:\\n',df['Number_Open_Accounts'].unique(),'\\n')\n",
    "print('null values in Number_Open_Accounts:',df['Number_Open_Accounts'].isnull().sum())\n",
    "#No issues :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in feature Total_Accounts:\n",
      " [ 14  24  16  22  30  15  12  31   9  43  13  17  28  26  21  20  34  27\n",
      "  10  44  25   8   7  19  41  18  11  65   6  29  23  37  32  45  38  36\n",
      "  71  40  42  51  53  35  47  39  54  46  33  58  49  59  62   4  69  60\n",
      "  52  56  48   5  63   3  50  55  61  57  64  76   2  75  67  66  70  77\n",
      "  86  68  72  90  89  81  98  73 105  74  79  85  82  80  93  78  87  88\n",
      "  91  84  83  94 156 102 116  97  92  99] \n",
      "\n",
      "null values in Total_Accounts: 0\n"
     ]
    }
   ],
   "source": [
    "print('unique values in feature Total_Accounts:\\n',df['Total_Accounts'].unique(),'\\n')\n",
    "print('null values in Total_Accounts:',df['Total_Accounts'].isnull().sum())\n",
    "#No issues :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique values in feature Gender:\n",
      " ['Female' 'Male'] \n",
      "\n",
      "null values in Gender: 0\n"
     ]
    }
   ],
   "source": [
    "# Problem 11:\n",
    "\n",
    "print('unique values in feature Gender:\\n',df['Gender'].unique(),'\\n')\n",
    "print('null values in Gender:',df['Gender'].isnull().sum())\n",
    "#Label encoding needed/get_dummies,No other issues :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not verified' 'VERIFIED - income' 'VERIFIED - income source'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Problem 12:\n",
    "print(df1['Income_Verified'].unique(),'\\n')\n",
    "# Fix verified-income,both indicate the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All label encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st problem solved\n",
    "df1['Loan_Amount_Requested']=df1['Loan_Amount_Requested'].apply(lambda x:x.replace(',','')).astype('int')\n",
    "\n",
    "# 2nd problem solved.\n",
    "df1['Length_Employed']=df1['Length_Employed'].replace('< 1 year','0 year')\n",
    "df1['Length_Employed']=df1['Length_Employed'].fillna('10+ years')\n",
    "df1['Length_Employed']=df1['Length_Employed'].str.findall('\\d+').str[0]\n",
    "df1['Length_Employed']=df1['Length_Employed'].astype('int')\n",
    "\n",
    "# 3rd problem solved.\n",
    "df1['Home_Owner'].replace('None',np.nan,inplace=True)\n",
    "df1['Home_Owner'].fillna('Mortgage',inplace=True)\n",
    "\n",
    "# problem 4 solved.\n",
    "df1['Annual_Income']=df1['Annual_Income'].fillna(df1['Annual_Income'].median())\n",
    "\n",
    "# Problem 8 solved:\n",
    "df1['Months_Since_Deliquency']=df1['Months_Since_Deliquency'].fillna(df1['Months_Since_Deliquency'].median())\n",
    "\n",
    "# Problem 12 solved:\n",
    "df1['Income_Verified'].replace({'VERIFIED - income':'verified','VERIFIED - income source':'verified'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop('Loan_ID',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 13, 'max_depth': 9, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5104375874870671"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Automatic Label encoding[All categorical label encoded]:\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "for i in df1.select_dtypes(include='object'):\n",
    "    df1[i]=le.fit_transform(df1[i])\n",
    "\n",
    "X=df1.drop('Interest_Rate',1)\n",
    "y=df1['Interest_Rate']\n",
    "\n",
    "X=df1.drop('Interest_Rate',1)\n",
    "y=df1['Interest_Rate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state =0)\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_params = {'max_depth':np.arange(1,10), 'min_samples_leaf':np.arange(2,30), 'criterion':['entropy','gini']}\n",
    "rscv = RandomizedSearchCV(dt, dt_params, cv=5)\n",
    "rscv.fit(X, y)\n",
    "print(rscv.best_params_)\n",
    "rscv_best_DT=rscv.best_params_\n",
    "DT=DecisionTreeClassifier(**rscv_best_DT)\n",
    "DT.fit(X_train,y_train)\n",
    "\n",
    "y_pred=DT.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)\n",
    "\n",
    "#{'min_samples_leaf': 13, 'max_depth': 9, 'criterion': 'entropy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Test data manipulations:\n",
    "\n",
    "dfte=pd.read_csv('D:/datasets+minipro/analytics vidhya datasets/janatahack p5 banking/test.csv')\n",
    "\n",
    "df2=dfte.copy()\n",
    "\n",
    "# 1st problem solved\n",
    "df2['Loan_Amount_Requested']=df2['Loan_Amount_Requested'].apply(lambda x:x.replace(',','')).astype('int')\n",
    "\n",
    "# 2nd problem solved.\n",
    "df2['Length_Employed']=df2['Length_Employed'].replace('< 1 year','0 year')\n",
    "df2['Length_Employed']=df2['Length_Employed'].fillna('10+ years')\n",
    "df2['Length_Employed']=df2['Length_Employed'].str.findall('\\d+').str[0]\n",
    "df2['Length_Employed']=df2['Length_Employed'].astype('int')\n",
    "\n",
    "# 3rd problem solved.\n",
    "df2['Home_Owner'].replace('None',np.nan,inplace=True)\n",
    "df2['Home_Owner'].fillna('Mortgage',inplace=True)\n",
    "\n",
    "# problem 4 solved.\n",
    "df2['Annual_Income']=df2['Annual_Income'].fillna(df2['Annual_Income'].median())\n",
    "\n",
    "# Problem 8 solved:\n",
    "df2['Months_Since_Deliquency']=df2['Months_Since_Deliquency'].fillna(df2['Months_Since_Deliquency'].median())\n",
    "\n",
    "# Problem 12 solved:\n",
    "df2['Income_Verified'].replace({'VERIFIED - income':'verified','VERIFIED - income source':'verified'},inplace=True)\n",
    "\n",
    "# Dropping unwanted variable\n",
    "df2.drop('Loan_ID',1,inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "for i in df2.select_dtypes(include='object'):\n",
    "    df2[i]=le.fit_transform(df2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Using Logistic Regression:[For all label encoded]\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "y_test_pred_lr=lr.predict(df2)\n",
    "finalpred=pd.concat([dfte['Loan_ID'],pd.DataFrame(y_test_pred_lr,columns=['Interest_Rate'])],1)\n",
    "finalpred.to_csv(\"D:/datasets+minipro/analytics vidhya datasets/janatahack p5 banking/lrpred.csv\",index=False)\n",
    "# This is the worst performing model with accuracy .40,we will move to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 18, 'max_depth': 9, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "##### Using Decision Tree:[For all label encoded]\n",
    "\n",
    "X=df1.drop('Interest_Rate',1)\n",
    "y=df1['Interest_Rate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state =0)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_params = {'max_depth':np.arange(1,10), 'min_samples_leaf':np.arange(2,30), 'criterion':['entropy','gini']}\n",
    "rscv = RandomizedSearchCV(dt, dt_params, cv=5)\n",
    "rscv.fit(X, y)\n",
    "print(rscv.best_params_)\n",
    "rscv_best_DT=rscv.best_params_\n",
    "DT=DecisionTreeClassifier(**rscv_best_DT)# These hyperparameters were obtained after randomizedsearch CV\n",
    "DT.fit(X,y)\n",
    "y_test_pred_DT=DT.predict(df2)\n",
    "finalpred=pd.concat([dfte['Loan_ID'],pd.DataFrame(y_test_pred_DT,columns=['Interest_Rate'])],1)\n",
    "finalpred.to_csv(\"D:/datasets+minipro/analytics vidhya datasets/janatahack p5 banking/dtpred.csv\",index=False)\n",
    "\n",
    "\n",
    "#{'min_samples_leaf': 16, 'max_depth': 8, 'criterion': 'gini'} -0.49 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB Classifier:[for all label encoded][nest results with below config]\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "params = {\"objective\": \"multi:softmax\",\"booster\": \"gbtree\", \"nthread\": 4, \"silent\": 1,\n",
    "                \"eta\": 0.08, \"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.7,\n",
    "                \"min_child_weight\": 1, \"num_class\": 3,\n",
    "                \"seed\": 2016, \"tree_method\": \"exact\"}\n",
    "\n",
    "xgb=XGBClassifier(learning_rate=0.09,n_estimators=125,max_depth=4,min_child_weight=4,colsample_bytree=0.5,reg_alpha=0.000001 )\n",
    "xgb=XGBClassifier(**params)\n",
    "xgb.fit(X,y)\n",
    "\n",
    "y_test_pred_xgb=xgb.predict(df2)\n",
    "finalpred=pd.concat([dfte['Loan_ID'],pd.DataFrame(y_test_pred_xgb,columns=['Interest_Rate'])],1)\n",
    "finalpred.to_csv(\"D:/datasets+minipro/analytics vidhya datasets/janatahack p5 banking/xgbpred.csv\",index=False)\n",
    "\n",
    "# Giving .5266 as result on AV platform.[so far this config is giving best results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using all dummies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### All get_dummies:\n",
    "\n",
    "df3=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st problem solved\n",
    "df3['Loan_Amount_Requested']=df3['Loan_Amount_Requested'].apply(lambda x:x.replace(',','')).astype('int')\n",
    "\n",
    "# 2nd problem solved.\n",
    "df3['Length_Employed']=df3['Length_Employed'].replace('< 1 year','0 year')\n",
    "df3['Length_Employed']=df3['Length_Employed'].fillna('10+ years')\n",
    "df3['Length_Employed']=df3['Length_Employed'].str.findall('\\d+').str[0]\n",
    "df3['Length_Employed']=df3['Length_Employed'].astype('int')\n",
    "\n",
    "# 3rd problem solved.\n",
    "df3['Home_Owner'].replace('None',np.nan,inplace=True)\n",
    "df3['Home_Owner'].fillna('Mortgage',inplace=True)\n",
    "\n",
    "# problem 4 solved.\n",
    "df3['Annual_Income']=df3['Annual_Income'].fillna(df3['Annual_Income'].median())\n",
    "\n",
    "# Problem 8 solved:\n",
    "df3['Months_Since_Deliquency']=df3['Months_Since_Deliquency'].fillna(df3['Months_Since_Deliquency'].median())\n",
    "\n",
    "# Problem 12 solved:\n",
    "df3['Income_Verified'].replace({'VERIFIED - income':'verified','VERIFIED - income source':'verified'},inplace=True)\n",
    "\n",
    "# Dropping unwanted variable\n",
    "df3.drop('Loan_ID',1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df3.select_dtypes(include='object'):\n",
    "    df3=pd.concat([df3,pd.get_dummies(df3[i],prefix=i,drop_first=True)],1)\n",
    "    del df3[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Test data manipulations:\n",
    "\n",
    "dfte=pd.read_csv('D:/datasets+minipro/analytics vidhya datasets/janatahack p5 banking/test.csv')\n",
    "\n",
    "df4=dfte.copy()\n",
    "\n",
    "# 1st problem solved\n",
    "df4['Loan_Amount_Requested']=df4['Loan_Amount_Requested'].apply(lambda x:x.replace(',','')).astype('int')\n",
    "\n",
    "# 2nd problem solved.\n",
    "df4['Length_Employed']=df4['Length_Employed'].replace('< 1 year','0 year')\n",
    "df4['Length_Employed']=df4['Length_Employed'].fillna('10+ years')\n",
    "df4['Length_Employed']=df4['Length_Employed'].str.findall('\\d+').str[0]\n",
    "df4['Length_Employed']=df4['Length_Employed'].astype('int')\n",
    "\n",
    "# 3rd problem solved.\n",
    "df4['Home_Owner'].replace('None',np.nan,inplace=True)\n",
    "df4['Home_Owner'].fillna('Mortgage',inplace=True)\n",
    "\n",
    "# problem 4 solved.\n",
    "df4['Annual_Income']=df4['Annual_Income'].fillna(df4['Annual_Income'].median())\n",
    "\n",
    "# Problem 8 solved:\n",
    "df4['Months_Since_Deliquency']=df4['Months_Since_Deliquency'].fillna(df4['Months_Since_Deliquency'].median())\n",
    "\n",
    "# Problem 12 solved:\n",
    "df4['Income_Verified'].replace({'VERIFIED - income':'verified','VERIFIED - income source':'verified'},inplace=True)\n",
    "\n",
    "# Dropping unwanted variable\n",
    "df4.drop('Loan_ID',1,inplace=True)\n",
    "\n",
    "for i in df4.select_dtypes(include='object'):\n",
    "    df4=pd.concat([df4,pd.get_dummies(df4[i],prefix=i,drop_first=True)],1)\n",
    "    del df4[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df3.drop('Interest_Rate',1)\n",
    "y=df3['Interest_Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Using Logistic Regression:[For all get dummies]\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "y_test_pred_lr=lr.predict(df4)\n",
    "finalpred=pd.concat([dfte['Loan_ID'],pd.DataFrame(y_test_pred_lr,columns=['Interest_Rate'])],1)\n",
    "finalpred.to_csv(\"D:/datasets+minipro/analytics vidhya datasets/janatahack p5 banking/lrpreddumm.csv\",index=False)\n",
    "# This is the worst performing model with accuracy .40,we will move to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 23, 'max_depth': 9, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "##### Using Decision Tree:[For all get dummies]\n",
    "\n",
    "X=df3.drop('Interest_Rate',1)\n",
    "y=df3['Interest_Rate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state =0)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_params = {'max_depth':np.arange(1,10), 'min_samples_leaf':np.arange(2,30), 'criterion':['entropy','gini']}\n",
    "rscv = RandomizedSearchCV(dt, dt_params, cv=5)\n",
    "rscv.fit(X, y)\n",
    "print(rscv.best_params_)\n",
    "rscv_best_DT=rscv.best_params_\n",
    "DT=DecisionTreeClassifier(**rscv_best_DT)# These hyperparameters were obtained after randomizedsearch CV\n",
    "DT.fit(X,y)\n",
    "y_test_pred_DT=DT.predict(df4)\n",
    "finalpred=pd.concat([dfte['Loan_ID'],pd.DataFrame(y_test_pred_DT,columns=['Interest_Rate'])],1)\n",
    "finalpred.to_csv(\"D:/datasets+minipro/analytics vidhya datasets/janatahack p5 banking/dtpreddumm.csv\",index=False)\n",
    "\n",
    "#{'min_samples_leaf': 4, 'max_depth': 9, 'criterion': 'gini'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB Classifier:[for all get dummies][nest results with below config]\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "params = {\"objective\": \"multi:softmax\",\"booster\": \"gbtree\", \"nthread\": 4, \"silent\": 1,\n",
    "                \"eta\": 0.08, \"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.7,\n",
    "                \"min_child_weight\": 1, \"num_class\": 3,\n",
    "                \"seed\": 2016, \"tree_method\": \"exact\"}\n",
    "\n",
    "xgb=XGBClassifier(learning_rate=0.09,n_estimators=125,max_depth=4,min_child_weight=4,colsample_bytree=0.5,reg_alpha=0.000001 )\n",
    "xgb=XGBClassifier(**params)\n",
    "xgb.fit(X,y)\n",
    "\n",
    "y_test_pred_xgb=xgb.predict(df4)\n",
    "finalpred=pd.concat([dfte['Loan_ID'],pd.DataFrame(y_test_pred_xgb,columns=['Interest_Rate'])],1)\n",
    "finalpred.to_csv(\"D:/datasets+minipro/analytics vidhya datasets/janatahack p5 banking/xgbpreddumm.csv\",index=False)\n",
    "\n",
    "# Giving .5266 as result on AV platform.[so far this config is giving best results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So,The best model is xgb with best accuracy in terms of f1 score as evaluation metric on analytics vidhya platform\n",
    "#### with all data being label encoded and making few corrections in given set of data by data cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
